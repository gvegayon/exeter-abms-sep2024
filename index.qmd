---
title: "Machine Learning Sandwich for Agent-Based Models"
subtitle: "Automatic calibration and post-processing adjustment for ABMs"
author: George G. Vega Yon, Ph.D.
institute: The University of Utah
format:
  revealjs:
    footer: "George G. Vega Yon, Ph.D. -- [ggv.cl/slides/exeter-abm-2024](https://ggv.cl/slides/exeter-abm-2024)"
    self-contained-math: true
    theme: ["default", "style.scss"]
    title-slide-attributes: 
      data-background-image: 'fig/Zoom-Background_PrideU.jpg'
      data-background-opacity: '0.2'
      data-background-size: 'contain'
    slide-number: c
bibliography: ["references.bib"]
---

# Motivation {background-color="black"}

## Motivation: Agent-Based Models in Science and Policy

::: {.incremental}
- Agent-Based Models [ABMs] are an important tool for scientific research and **policy-making**:
  - For scenario modeling: "What if we do X?"
  - For forecasting: "How many cases will we have in the next week?"

- {{< fa compass-drafting >}} Calibration can be time-consuming: grid search, Markov models, etc.

- There's **no consensus on the best calibration approach**.

- {{< fa chart-line >}} ABMs (and models in general) are **limited in their predictive power**.
:::

::: {layout-ncol="2" layout-valign="center"}
![](fig/chicago-btm-fig-2.png.avif){width="60%" fig-align="center"}

Schematic of compartmental and ABMs used by the US Centers for Disease and Control Prevention during a Measles Outbreak Response @cdcModelHowDisease2024.
:::

## Machine Learning Sandwich for ABMs

![](fig/mechml-overview.svg){fig-align="center" width="90%"}

# Using ML for Calibration

## Using ML for Calibration

![](fig/calibration-is-hard.png){width="90%" fig-align="center"}

## Using ML for Calibration: A proposal

::: {.incremental}
1. Specify the model to be calibrated, for example, a SIR model. We will denote the model $\mathcal{M}: \boldsymbol{\theta} \to \mathbf{Y}$.

2. Identify the model parameters $\boldsymbol{\theta}$ that will be calibrated. In the case of a SIR model, we may want to specify the contact, infection, and recovery rates.

3. Simulate the model $N$ times drawing $\boldsymbol{\theta}$ from a prior distribution. The simulation procedure follows: For $i \in \{1,\dots,N\}$ do
    - Draw $\boldsymbol{\theta}_i \sim \Theta_{\mbox{prior}}$; with a uniform distribution when no prior information is available.
    - Draw $y_i$ from $\mathcal{M}(\boldsymbol{\theta}_i)$ and store it.
    - next simulation.
  
    We will have our training dataset $T\equiv\left\{\boldsymbol{\theta}_i, y_i\right\}_{i=1}^N$.
  
4. Finally, we can use the training set to generate the mapping $\mathcal{C}: \boldsymbol{Y} \to \boldsymbol{\theta}$.
:::

::: {.fragment style="font-size: 150%" .callout}
With the calibrated model $\mathcal{C}$, we can answer the question *"What parameters $\theta$ better fit the observed data?"*.
:::

# Mechanistic Machine Learning {background-color="black"}

## Machine Learning is Broken

::: {.fragment}
- After all the data pouring, attention to causal inference and mechanistic models is coming back [@bakerMechanisticModelsMachine2018; @pearlSevenToolsCausal2019]
:::

::: {.fragment}
- The case of Google Flu Trends: Paper reported a 0.97 correlation [@ginsbergDetectingInfluenzaEpidemics2009], but predictions overshoot by 100% [@kandulaReappraisingUtilityGoogle2019; @lazerParableGoogleFlu2014].
:::


![](fig/google-flu.png){width="90%" fig-align="center"}

## Mechanistic Machine Learning

Mechanistic Machine Learning [MechML]--*a.k.a.* theory-guided data science/machine learning: A hybrid between theory and data-driven prediction.

::: {.columns style="font-size: 150%"}
::: {.column width="45%" .callout .fragment}
### Mechanistic models

- Inference-driven (causality).
- Great for small datasets.
- Knowledge beyond the observed data.
:::
::: {.column width="45%" .callout .fragment}
### Machine Learning

- Data-driven (prediction).
- Great for big data.
- Finds hidden knowledge in observed data.
:::
:::

::: {.fragment}
ML can help explain what theory hasn't... but we still need theory [@lazerParableGoogleFlu2014]!
:::

## MechML: State-of-the-art

::: {.incremental}
- Adjusting errors in mechanistic-based prediction models (like ABMs). [@compagniHybridNeuralNetworkSEIR2022]

- Incorporating mechanistically inferred data as an additional -omics layer. [@zampieriMachineDeepLearning2019]

- Using pathway networks to add "external knowledge" as features. [@altaweraqiImprovedPredictionGene2022]

- Creating a loss function with a mechanistic penalty for modeling tumor cell density [@gawIntegrationMachineLearning2019]

- Using simulations to inform neural networks for epidemic forecasting [@wangTDEFSITheoryguidedDeep2020].

- and more [@jornerMachineLearningMeets2021; @willardIntegratingScientificKnowledge2022a; @jiaPhysicsGuidedMachineLearning2021; @vonruedenInformedMachineLearning2023]
:::

::: {.callout-warning .fragment style="font-size: 120%"}
1. Mechanistic Machine Learning **is not** domain-knowledge-aided feature engineering. You need a whole other model to complement the ML algorithm.

2. This isn't just an ML ensemble; you must have an ML and a Mech model.
:::

# How to MechML? {background-color="black"}

## How to MechML: Correcting prediction errors

::: {}
![](fig/mechml-errors.svg){fig-align="center" width="50%"}
:::

Train a model that predicts ABM forecast errors. The model takes the following form:

$$
Loss(\boldsymbol{\omega}): \left\{\mathcal{M}(\boldsymbol{\theta}), \boldsymbol{x}\right\} \to \widehat{\boldsymbol{\varepsilon}} \equiv \left\lVert \widehat{\boldsymbol{\varepsilon}} - f\left(\boldsymbol{\omega}, \mathcal{M}(\boldsymbol{\theta}), \boldsymbol{x}\right)\right\rVert{}_p,
$$

where $\widehat{\boldsymbol{\varepsilon}}\equiv \left(\mathcal{M}(\boldsymbol{\theta}) - y_{obs}\right)$ ABM forecast error, $\boldsymbol{y}_{obs}$ is the observed data, $f(\cdot)$ is a non-linear function, $\boldsymbol{x}$ are additional features for the model, and $\boldsymbol{\omega}$ is an array of weights associated with the ML o predict $\boldsymbol{\varepsilon}$.

## How to MechML: Predictions as feature

::: {}
![](fig/mechml-feature.svg){fig-align="center" width="50%"}
:::

Use the ABM predictions as a feature in the ML model. The model takes the following form:

$$
Loss(\boldsymbol{\omega}) \equiv \left\lVert \boldsymbol{y}_{obs} - f\left(\boldsymbol{\omega}, \mathcal{M}(\boldsymbol{\theta}), \boldsymbol{x}\right)\right\rVert{}_p,
$$

## How to MechML: Mechanistic penalty

::: {}
![](fig/mechml-penalty.svg){fig-align="center" width="50%"}
:::

Use a mechanistic penalty in the ML loss function. The model takes the following form:

$$
Loss(\boldsymbol{\omega}) \equiv \left\lVert \boldsymbol{y}_{obs} - f\left(\boldsymbol{\omega}, \boldsymbol{x}\right)\right\rVert{}_p + \lambda \left\lVert f\left(\boldsymbol{\omega}, \boldsymbol{x}\right) - \mathcal{M}(\theta)\right\rVert{}_p,
$$

where $\lambda$ is a hyperparameter that controls the weight of the mechanistic penalty.

# Preliminary results in calibration {{< fa compass-drafting >}} {background-color="black"}

## Calibration {{< fa compass-drafting >}}

Susceptible-Infected Recovered model:

::: {layout-ncol="2" .incremental}
- Agent Based Model with `epiworldR`.
- Fully connected graph (everyone is connected to everyone).
- We ran 20,000 simulations
- Parameters of interest: 
  - Initial state
  - Contact rate
  - Probability of transmission
  - Probability of recovery

```{r}
#| fig-asp: 1
#| echo: false
#| out-width: 70%
library(epiworldR)
set.seed(123)
truth <- c(.01, 8, .3, .3)
abm <- ModelSIRCONN(
  "mycon",
  prevalence        = truth[1],
  contact_rate      = truth[2],
  transmission_rate = truth[3],
  recovery_rate     = truth[4],
  n                 = 50000
)

verbose_off(abm)

run(abm, 100, 223)
op<-par(cex=3)
plot(abm, main="")
par(op)
```
:::


::: {style="font-size: 120%"}
We trained a convolutional neural network to build our calibrator:

$$
\mathcal{C}: \text{Epicurves} \to \boldsymbol{\theta}
$$

Where $\boldsymbol{\theta} = \left[\text{Init. state}\text{Contact Rate}, \text{P(transmit)}, \text{P(recover)}\right]$

:::

## Calibration  {{< fa compass-drafting >}}: CNN Architecture

![](fig/CNN.drawio.svg){fig-align="center" width="90%"}

## Calibration  {{< fa compass-drafting >}}: Model fit

![](fig/sir-cnn.png){fig-align="center" width="90%"}

## Calibration  {{< fa compass-drafting >}}: Example

::: {layout-ncol="2" .incremental  layout-valign="center"}

|Parameter    | Predicted| Truth|
|:------------|---------:|-----:|
|Init. state  |      0.01|  0.01|
|Contact Rate |      6.29|  8.00|
|P(transmit)  |      0.33|  0.30|
|P(recover)   |      0.33|  0.30|

- Simple model does an OK job recovering the parameters.
- Non-unique solution, e.g., lower contact rate and higher transmission rate.
- Model does not deal with important factors: 
  - What if the number of agents is 1,000,000?
  - What if there's a policy/behavior change?
:::
::: {}
```{r}
#| fig-asp: 0.7
#| echo: false
#| out-width: 55%
#| fig-align: center
op<-par(cex=2, mai=c(4,4,0,0),lwd=2)
plot(abm, main="")
par(op)
```
:::

# Example results in prediction {{< fa chart-line >}} {background-color="black"}


## Prediction: Model of gene functions (0/1)

Example model by speaker using mechanistic machine learning to improve accuracy in prediction of gene functions.

::: {.columns}
::: {.column width="30%" .incremental}
- Used a mechanistic model of evolution of gene functions to predict presence/absence of them.
- ML model: logistic regression featuring a large number of parameters.
- MechML: use the mechanistic predictions as features in the logit.
:::
::: {.column width="70%"}
![](fig/logit-aucs-ols-aphylo.svg){fig-align="center"}
:::
:::

## Machine Learning Sandwich for ABMs (repeat)

![](fig/mechml-overview.svg){fig-align="center" width="90%"}


## Discussion

::: {.incremental}
- ABMs are a powerful tool for research and policy analysis.

- Overall, calibration and prediction are challenging tasks; the latter, even for advanced ML models.

- We propose a method that leverages the best of both worlds: mechanistic models and machine learning: **Machine Learning Sandwich for ~ABM~ Models**.

- The calibration step leverages artifical data for training.

- The forecasting step is addressed using Mechanistic Machine Learning [MechML]. At least three ways to apply it:

  - ML to correct for systematic biases.

  - Mech predictions as features in an ML algorithm.

  - A Mech penalty embedded in the ML Loss function.

- Finally, we presented preliminary results on calibration of an SIR model and predictions using MechML for gene function prediction.
:::

::: {.fragment style="text-align: center;"}
### Thank you!

George G. Vega Yon, Ph.D.<br>
The University of Utah<br>
<https://ggvy.cl>
:::

## References
